{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_cleaning(preprocessing_nltk).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fXV7sjsIsc6",
        "outputId": "dc80add7-3109-4445-b65e-acf7a5e5dd8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FTkfbMPI8pP",
        "outputId": "0a7dca44-cb6f-4ae3-869d-7065e85181d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Creating bunch of sentences\n",
        "raw_docs = [\"#KKR Should Remember That They,Were Best Only Till ;tHey hAd left handed batsman as captain\"]\n",
        "import string\n",
        "raw_docs = [doc.lower() for doc in raw_docs]\n",
        "print(raw_docs)\n",
        "\n",
        "\n",
        "# word tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
        "print(tokenized_docs)\n",
        "\n",
        "# Removing punctuation\n",
        "import re\n",
        "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\n",
        "tokenized_docs_no_punctuation = []\n",
        "\n",
        "for review in tokenized_docs:\n",
        "    new_review = []\n",
        "    for token in review:\n",
        "        new_token = regex.sub(u'', token)\n",
        "        if not new_token == u'':\n",
        "            new_review.append(new_token)\n",
        "    \n",
        "    tokenized_docs_no_punctuation.append(new_review)\n",
        "    \n",
        "print(tokenized_docs_no_punctuation)\n",
        "\n",
        "# Cleaning text of stopwords\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "tokenized_docs_no_stopwords = []\n",
        "\n",
        "for doc in tokenized_docs_no_punctuation:\n",
        "    new_term_vector = []\n",
        "    for word in doc:\n",
        "        if not word in stopwords.words('english'):\n",
        "            new_term_vector.append(word)\n",
        "    \n",
        "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
        "\n",
        "print(tokenized_docs_no_stopwords)\n",
        "\n",
        "# Stemming and Lemmatization\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "wordnet = WordNetLemmatizer()\n",
        "\n",
        "preprocessed_docs = []\n",
        "\n",
        "for doc in tokenized_docs_no_stopwords:\n",
        "    final_doc = []\n",
        "    for word in doc:\n",
        "        #final_doc.append(porter.stem(word))\n",
        "        final_doc.append(wordnet.lemmatize(word))\n",
        "    \n",
        "    preprocessed_docs.append(final_doc)\n",
        "\n",
        "print(preprocessed_docs)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#kkr should remember that they,were best only till ;they had left handed batsman as captain']\n",
            "[['#', 'kkr', 'should', 'remember', 'that', 'they', ',', 'were', 'best', 'only', 'till', ';', 'they', 'had', 'left', 'handed', 'batsman', 'as', 'captain']]\n",
            "[['kkr', 'should', 'remember', 'that', 'they', 'were', 'best', 'only', 'till', 'they', 'had', 'left', 'handed', 'batsman', 'as', 'captain']]\n",
            "[['kkr', 'remember', 'best', 'till', 'left', 'handed', 'batsman', 'captain']]\n",
            "[['kkr', 'remember', 'best', 'till', 'left', 'handed', 'batsman', 'captain']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5EmWYMkKXgo"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7dZ32-NJe8m"
      },
      "source": [
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}